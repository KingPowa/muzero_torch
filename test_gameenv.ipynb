{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym as g\n",
    "from gym import spaces\n",
    "from connect4 import *\n",
    "from envs import ConnectNEnv, AdvancedDiscreteEnv\n",
    "from networks.architecture import RepresentationNetwork, DynamicsNetwork, PredictionNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ConnectNEnv()\n",
    "env.single_obs_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observations': array([[0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0]], dtype=int8),\n",
       "  'action_mask': array([1, 1, 1, 1, 1, 1, 1], dtype=int8),\n",
       "  'player_1_board': array([[0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0]], dtype=int8),\n",
       "  'player_2_board': array([[0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]], dtype=int8),\n",
       "  'current_player': 'P2'},\n",
       " 0.0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that acts as an adapter for an env for the Muzero\n",
    "class Game:\n",
    "\n",
    "    def __init__(self, game_env: AdvancedDiscreteEnv, history_len = 7, only_once_per_player = False):\n",
    "        self.game_env = game_env\n",
    "        # Parameter to describe if the game has ended\n",
    "        self.is_done = False\n",
    "        # How many version of the board should be encoded in the state\n",
    "        self.history_len = history_len\n",
    "        # History of states\n",
    "        self.history = {\n",
    "            \"P1\": [],\n",
    "            \"P2\": []\n",
    "        }\n",
    "        # History of actions\n",
    "        self.action_history = []\n",
    "        # history of reward\n",
    "        self.rewards = []\n",
    "        # Current player\n",
    "        self.current_player = \"P1\"\n",
    "        # Parameter that tells if the state representation (and so, the history) will save the board also for the other player\n",
    "        # when it's not its turn.\n",
    "        self.only_once_per_player = only_once_per_player\n",
    "\n",
    "    def terminal(self):\n",
    "        \"\"\" Boolean which informs\n",
    "        whether the game has ended\n",
    "        \"\"\"\n",
    "        return len(self.game_env.possible_actions) > 0 and self.is_done\n",
    "    \n",
    "    def make_image(self, idx):\n",
    "        \"\"\" Returns the observation at the given index\n",
    "        where idx means how many steps in the past\n",
    "        \"\"\"\n",
    "        sub_history_P1 = self._make_subhistory(idx, \"P1\")\n",
    "        sub_history_P2 = self._make_subhistory(idx, \"P2\")\n",
    "        \n",
    "        return torch.cat((torch.cat(sub_history_P1, dim=1), torch.cat(sub_history_P2, dim=1),), dim=1)\n",
    "    \n",
    "    def _make_subhistory(self, idx, player):\n",
    "        idx = len(self.history[player]) + idx if idx < 0 else idx\n",
    "        sub_history = self.history[player][max(0, idx - self.history_len + 1): idx + 1]\n",
    "        left_history = self.history_len-len(sub_history)\n",
    "        if left_history > 0:\n",
    "            sub_history.append(torch.zeros((1, left_history, self.game_env.single_obs_size[2], self.game_env.single_obs_size[3],)))\n",
    "        return sub_history\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" Make env advance of 1 step and understand if the game is ended\n",
    "        \"\"\"\n",
    "        observation_dict, reward, self.is_done, _ = self.game_env.step(action)\n",
    "        self.action_history.append(action)\n",
    "        self.current_player = observation_dict[\"current_player\"]\n",
    "        if self.only_once_per_player:\n",
    "            self.history[self.current_player].append(torch.Tensor(observation_dict[self._board_name(self.current_player)]).view(self.game_env.single_obs_size))\n",
    "        else:\n",
    "            self.history[\"P1\"].append(torch.Tensor(observation_dict[\"player_1_board\"]).view(self.game_env.single_obs_size))\n",
    "            self.history[\"P2\"].append(torch.Tensor(observation_dict[\"player_2_board\"]).view(self.game_env.single_obs_size))\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        self.current_player = observation_dict[\"current_player\"]\n",
    "\n",
    "    def _board_name(self, player):\n",
    "        return \"player_1_board\" if player == \"P1\" else \"player_2_board\"\n",
    "\n",
    "    def to_play(self):\n",
    "        \"\"\" Return next current player \n",
    "        \"\"\"\n",
    "        return self.current_player\n",
    "    \n",
    "    def action_history(self):\n",
    "        \"\"\" Return list of executed action\n",
    "        \"\"\"\n",
    "        return self.action_history\n",
    "    \n",
    "    def legal_actions(self):\n",
    "        \"\"\" Return a list of legal actions\n",
    "        \"\"\"\n",
    "        return self.game_env.possible_actions\n",
    "    \n",
    "    def action_mask(self):\n",
    "        \"\"\" Return an action mask\n",
    "        \"\"\"\n",
    "        action_mask = self.game_env.action_mask\n",
    "        return torch.Tensor(action_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(ConnectNEnv(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(1)\n",
    "game.make_image(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<architecture.game.Game at 0x1779e7a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from architecture.game import Game\n",
    "\n",
    "Game(ConnectNEnv, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
