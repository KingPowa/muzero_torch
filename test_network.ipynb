{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 4]) torch.Size([1, 256, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Representation Neural Network\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch.nn import Conv2d, Linear\n",
    "from torch.nn import ReLU, BatchNorm2d\n",
    "\n",
    "class ConvolutionalBlock(Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_channels = 256, kernel_size = (3,3), stride=(1,1), padding=(1,1)):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        self.convo = Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.norm = BatchNorm2d(num_features=n_channels)\n",
    "        self.act = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.convo(x)))\n",
    "    \n",
    "x = torch.randn((1,1,3,4), device=device)\n",
    "convo_block = ConvolutionalBlock(1).to(device=device)\n",
    "\n",
    "print(x.shape, convo_block(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 4]) torch.Size([1, 256, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_channels = 256, kernel_size = (3,3), stride=(1,1)):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.convo1 = ConvolutionalBlock(in_channels=in_channels, n_channels=n_channels, kernel_size=kernel_size, stride=stride)\n",
    "        self.convo2 = Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=(1,1), stride=stride)\n",
    "        self.norm = BatchNorm2d(num_features=n_channels)\n",
    "        self.act = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_conn = x\n",
    "        x = self.convo1(x)\n",
    "        x = self.norm(self.convo2(x))\n",
    "        x = x + skip_conn\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "x = torch.randn((1,1,3,4), device=device)\n",
    "convo_block = ResidualBlock(1).to(device=device)\n",
    "\n",
    "print(x.shape, convo_block(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 6, 7])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GenericResidualNetwork(Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_channels = 256, n_layers=10, kernel_size = (3,3)):\n",
    "        super(GenericResidualNetwork, self).__init__()\n",
    "\n",
    "        # For now, simple, no downscale\n",
    "        self.input_layer = ResidualBlock(in_channels=in_channels, n_channels=n_channels, kernel_size=kernel_size)\n",
    "\n",
    "        self.residuals = ModuleList([ResidualBlock(in_channels=n_channels, n_channels=n_channels, kernel_size=kernel_size) for _ in range(n_layers-1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.input_layer(x)\n",
    "        for res_layer in self.residuals:\n",
    "            x = res_layer(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((1,1,6,7), device=device)\n",
    "convo_block = GenericResidualNetwork(1).to(device=device)\n",
    "\n",
    "x = convo_block(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPredictor(Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_conv = 2 ):\n",
    "        \n",
    "        self.conv1 = Conv2d() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 601])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ContinousValuePredictor(Module):\n",
    "\n",
    "    def __init__(self, in_channels, board_total_slots, n_outputs, n_convs = 2):\n",
    "        super(ContinousValuePredictor, self).__init__()\n",
    "\n",
    "        self.convos = ModuleList()\n",
    "        \n",
    "        for _ in range(n_convs):\n",
    "            out_channels = in_channels // 2\n",
    "            convo = ConvolutionalBlock(in_channels=in_channels, n_channels=out_channels, kernel_size=(1,1), padding=(0,0))\n",
    "            self.convos.append(convo)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.output_size = board_total_slots * out_channels\n",
    "        self.linear = Linear(self.output_size, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for conv in self.convos:\n",
    "            x = conv(x)\n",
    "\n",
    "        x = x.view(-1, self.output_size)\n",
    "        return self.linear(x)       \n",
    "\n",
    "x = torch.randn((1,256,6,7), device=device)\n",
    "convo_block = ContinousValuePredictor(256, 42, 601).to(device=device)\n",
    "\n",
    "x = convo_block(x)\n",
    "x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 601])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DynamicsNetwork(Module):\n",
    "\n",
    "    def __init__(self, in_channels, board_total_slots, n_convs = 2, n_channels = 256, n_residual_layers=10, kernel_size = (3,3)):\n",
    "        super(DynamicsNetwork, self).__init__()\n",
    "\n",
    "        self.first_net = GenericResidualNetwork(in_channels=in_channels, n_channels=n_channels, n_layers=n_residual_layers, kernel_size=kernel_size)\n",
    "        self.reward_predictor = ContinousValuePredictor(in_channels=n_channels, board_total_slots=board_total_slots, n_outputs=601, n_convs=n_convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.reward_predictor(self.first_net(x))\n",
    "    \n",
    "\n",
    "x = torch.randn((1,256,6,7), device=device)\n",
    "convo_block = DynamicsNetwork(256, 42).to(device=device)\n",
    "\n",
    "x = convo_block(x)\n",
    "x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1555, -0.8329,  0.1847, -0.1942,  0.3341,  0.2840,  0.0784,  0.1169,\n",
       "          -0.5542, -0.0793, -0.9286, -0.1915, -1.0725,  0.6287,  0.0216, -0.4392,\n",
       "           0.1499, -0.1900, -0.3710, -0.9801,  0.3121, -0.1348, -0.5337,  0.2209,\n",
       "          -0.2979, -0.0740, -0.5907,  0.3962,  0.5812, -0.2135, -0.1696,  0.0710,\n",
       "          -0.8737, -0.6410,  0.2240,  0.5494, -0.1721,  0.2564, -0.1549,  0.1967,\n",
       "           0.2573, -0.6970,  0.1928,  0.7517, -0.2378,  0.6223,  0.9036,  0.7385,\n",
       "           0.2385, -0.3269,  0.3727,  0.2111,  0.5860,  0.5001, -0.2817, -0.2401,\n",
       "          -0.2091,  0.0435,  0.4508, -0.4706, -0.1902,  0.1987,  0.5536,  0.0138,\n",
       "           0.3129,  0.3344,  0.4260,  0.6085, -0.7781, -0.4287, -0.7040, -0.4321,\n",
       "          -0.5182,  0.0289,  0.0201,  0.0366, -0.2440,  0.0718,  0.4529, -0.7286,\n",
       "          -0.0469, -0.2206,  0.4698,  0.3199, -0.4301,  0.3965,  0.4098,  0.0192,\n",
       "           0.3331,  0.5308,  0.5195, -0.4866, -0.2346,  0.2618, -0.2199, -0.3240,\n",
       "          -0.0446, -0.4233,  0.0598, -0.0109, -0.4859, -0.0768,  1.1191, -0.1171,\n",
       "           0.7012,  0.5398, -0.3831,  0.0344,  0.3882, -0.9768, -0.3709,  0.3917,\n",
       "           0.1367,  0.7933, -0.0372,  0.7226,  0.3467,  0.2110,  0.5955, -0.4607,\n",
       "          -0.6868,  0.0152,  0.1236, -0.0038, -0.5414, -0.1919,  0.1349,  0.3312,\n",
       "           0.9500, -0.2709, -0.6811,  0.3521, -0.9500,  0.6775, -0.0358, -0.6510,\n",
       "          -0.1975,  0.3557, -0.4112, -0.0690,  0.0209,  0.2287,  0.3606,  0.6550,\n",
       "          -0.3288, -0.4047, -0.6619, -0.0550,  0.0520,  0.4877, -0.5303, -0.2385,\n",
       "          -0.2300, -0.1794, -0.4225,  0.4858, -0.0838, -0.3491,  0.2130,  0.5680,\n",
       "           0.8053,  0.8452, -0.4730, -0.4062,  0.4647,  0.9330, -0.0594,  0.3015,\n",
       "           0.5351,  0.1515,  0.0061, -0.4313,  0.1753, -0.0320,  0.4538,  0.5921,\n",
       "           0.1405,  0.2421, -0.3716, -0.0045, -0.0626, -0.3105,  0.2248,  0.2171,\n",
       "           0.2324,  0.1712,  0.2345, -0.2863,  0.0310,  0.0113, -0.4259, -0.2885,\n",
       "           0.5173,  0.0873,  0.3794, -0.0775,  0.1413,  0.3667, -0.2258,  0.1642,\n",
       "          -0.4321,  0.1564, -0.3631,  0.4041,  0.3487,  0.6535, -0.3958,  0.1884,\n",
       "          -0.4799, -0.4694,  0.0547,  0.4798,  0.3167, -0.4583, -0.8305,  0.5875,\n",
       "          -0.1518,  0.4304, -0.2300,  0.0968, -0.4375,  0.1366,  0.8155,  0.1214,\n",
       "          -0.7764,  0.4865, -0.1499, -0.8258,  0.2537,  0.2036,  0.1737, -0.2857,\n",
       "           0.1418, -0.0693,  0.4055,  0.1711,  0.1108, -0.2658,  0.4611,  0.0210,\n",
       "          -0.1865, -0.2153, -0.4868,  0.2991, -0.2585, -0.2465, -0.0207,  0.4086,\n",
       "          -0.1683,  0.1773, -0.5139, -0.2241, -0.1064,  0.0704,  0.6416, -0.0109,\n",
       "          -0.9224,  0.0937,  0.6445, -0.4489,  1.1264, -0.0753, -0.2520,  0.0751,\n",
       "          -0.7856, -0.4518,  0.0081, -0.2184,  0.1746,  0.4603, -0.2228,  0.5504,\n",
       "           0.6409,  0.1256, -0.0443, -0.4949, -0.2717,  0.3771, -0.8506, -0.6131,\n",
       "           0.4815, -0.0518,  0.4027,  0.4933, -0.2009,  0.1742,  0.7262, -0.1191,\n",
       "           0.0457, -0.5739, -0.0564,  0.4069, -0.1628,  0.4850, -0.1137,  1.0728,\n",
       "           0.0618, -0.0571,  0.1045,  0.2599, -0.1891,  1.1733, -0.3405,  0.1627,\n",
       "           0.3627,  0.0954, -0.4080, -0.2854,  0.1454,  0.4804, -0.1588,  0.2094,\n",
       "           0.4676,  0.0587,  0.6931,  0.4921,  0.3306, -0.9062,  0.5892,  1.0094,\n",
       "           0.5053, -0.2705, -0.1426, -0.0036,  0.1020, -0.0478,  0.4214, -0.1933,\n",
       "          -1.0463,  0.4574,  0.4213,  0.2413, -0.1142,  0.5835, -0.0901, -0.2500,\n",
       "           0.1301, -0.1910,  0.4087, -0.9554, -0.1034,  0.3539,  0.2428, -0.2995,\n",
       "          -0.0896, -0.0679,  0.4457,  0.0660,  0.2516, -0.1447,  0.7232, -0.1871,\n",
       "           0.1606, -0.0519, -0.7243,  0.5757,  0.2575, -0.3615,  0.1634, -0.5936,\n",
       "           0.0619,  0.0551, -0.1004, -0.5114, -0.1601,  0.4021, -0.2525,  0.0108,\n",
       "           0.1427, -0.0458, -0.0884, -0.8491, -0.3159,  0.9239, -0.7695,  0.0149,\n",
       "          -0.0194,  0.2089,  0.2790,  0.6110, -0.4409, -0.1731,  0.2880, -0.0771,\n",
       "           0.0266, -0.0088,  0.4050,  0.1972,  0.0907,  0.4401, -0.5816,  0.7249,\n",
       "          -0.4113,  0.1236, -0.4503, -0.0690, -0.1719, -0.0884, -0.3356,  0.5872,\n",
       "          -0.1679,  0.2222,  0.2263,  0.0996, -0.8024,  0.2537,  0.1615, -0.2565,\n",
       "           0.4767, -0.4618,  0.2272,  0.7366, -0.2199, -0.2681, -0.2983, -0.3040,\n",
       "          -0.2270, -0.6947, -0.5928, -0.0248,  0.1084, -0.6835, -0.2031, -0.0068,\n",
       "          -0.4680,  0.3290,  0.0604, -0.2627,  0.2275, -0.0686, -0.3472,  1.1419,\n",
       "          -0.0666,  0.1285,  0.3752,  0.0073,  0.4447,  0.5045, -0.3993,  0.3513,\n",
       "          -0.4226, -0.0523, -0.3482, -0.0873, -0.4177, -0.8573,  0.2727, -0.2942,\n",
       "           0.3567,  0.1098,  0.2346, -0.0828, -0.5368,  0.0769, -0.2622, -0.2895,\n",
       "          -0.2534, -0.1829,  0.6204, -0.2974,  0.1358, -0.4633,  0.0670, -0.1799,\n",
       "          -0.4320,  0.1702, -0.3681,  0.1742,  0.1913, -0.0913,  0.4455, -0.1913,\n",
       "           0.0566, -0.2729,  0.1926, -0.3316,  0.2334, -0.1702, -0.2345,  0.0672,\n",
       "           0.3258, -0.1457,  0.3651,  0.0439,  0.2387,  0.0132,  0.0326, -0.0561,\n",
       "           0.0192, -0.1054, -0.6566, -0.4160,  0.7266, -0.2890, -0.1388,  0.6413,\n",
       "           0.3588, -0.1309,  0.2298, -0.0768,  0.0612, -0.1367, -0.0212, -0.0985,\n",
       "          -0.5478, -0.5191,  0.0945,  0.1402,  0.0614, -0.2402,  0.1642,  0.0400,\n",
       "           0.2824, -0.3613, -0.0456, -0.6595, -0.1470,  0.1160, -0.8064,  0.0575,\n",
       "           0.2692, -0.0997, -0.3616,  0.4421, -0.5000, -0.2303, -0.3042,  0.0131,\n",
       "           0.1205,  0.0732, -0.0672, -0.3169,  0.2288, -0.0439,  0.5732, -0.0688,\n",
       "           0.2244, -0.3692,  0.1557,  0.1504, -0.2485,  0.0042,  0.0772, -0.3303,\n",
       "          -0.2160,  0.4117, -0.7868, -0.2100,  0.1682,  0.6159,  0.2112, -1.3096,\n",
       "           0.2302, -0.1628, -0.1320,  0.3278, -0.4340,  0.1733,  0.9178, -0.0429,\n",
       "           0.3801, -0.1186, -0.4941, -0.6237, -1.0763, -0.1091,  0.1501, -1.0193,\n",
       "          -0.2526, -0.1737,  0.1841,  0.4516, -0.1353,  0.2798, -0.1796,  0.5874,\n",
       "           0.1936, -0.0901,  0.5301,  0.3934, -0.1717,  0.0964, -0.1198, -0.5750,\n",
       "           0.1329,  1.1693, -0.5164,  0.3162,  0.0640, -0.0698,  0.2774, -0.3327,\n",
       "           0.4438,  0.4589,  0.3838, -0.3517,  0.4374,  0.4457, -0.0446,  0.2700,\n",
       "           0.4110]], device='mps:0', grad_fn=<LinearBackward0>),\n",
       " tensor([[0.0547, 0.2055, 0.2234, 0.1332, 0.1388, 0.1768, 0.0676]],\n",
       "        device='mps:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PredictionNetwork(Module):\n",
    "\n",
    "    def __init__(self, in_channels, board_total_slots, action_space_size, n_convs = 2, n_channels = 256, n_residual_layers=10, kernel_size = (3,3)):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "\n",
    "        self.first_net = GenericResidualNetwork(in_channels=in_channels, n_channels=n_channels, n_layers=n_residual_layers, kernel_size=kernel_size)\n",
    "        self.value_predictor = ContinousValuePredictor(in_channels=n_channels, board_total_slots=board_total_slots, n_outputs=601, n_convs=n_convs)\n",
    "        self.policy_predictor = ContinousValuePredictor(in_channels=n_channels, board_total_slots=board_total_slots, n_outputs=action_space_size, n_convs=n_convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.first_net(x)\n",
    "        pp = self.policy_predictor(x)\n",
    "        return self.value_predictor(x), torch.nn.functional.softmax(pp, dim=1)\n",
    "    \n",
    "\n",
    "x = torch.randn((1,256,6,7), device=device)\n",
    "convo_block = PredictionNetwork(256, 42, 7).to(device=device)\n",
    "\n",
    "x = convo_block(x)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
